{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorboard\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "import scipy.io as io\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/mnt/raid/xyiheng/training/\"\n",
    "SAMPLE_SIZE = 18750\n",
    "BATCH_SIZE = 8\n",
    "NUM_CLASSES = 4\n",
    "NEAR_THRESHOLD = 1\n",
    "FAR_THRESHOLD = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_SIZE_ARR = torch.FloatTensor([[1.5260834319114884,1.6285898684851439,3.883954491684643],\n",
    "                                   [1.7607064854022731,0.6601894361488745,0.8422843770893693],\n",
    "                                   [1.73720344191764,0.5967732022126614,1.7635464044253226]]).to(device)\n",
    "RATIO = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_rigid_trans(Tr):\n",
    "    \"\"\" Inverse a rigid body transform matrix (3x4 as [R|t])\n",
    "        [R'|-R't; 0|1]\n",
    "    \"\"\"\n",
    "    inv_Tr = np.zeros_like(Tr)  # 3x4\n",
    "    inv_Tr[0:3, 0:3] = np.transpose(Tr[0:3, 0:3])\n",
    "    inv_Tr[0:3, 3] = np.dot(-np.transpose(Tr[0:3, 0:3]), Tr[0:3, 3])\n",
    "    return inv_Tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Dataset\n",
    "class KittiDataset(Dataset):\n",
    "    def __init__(self, train, dataset_path, sample_size, augment):\n",
    "        super(KittiDataset, self).__init__()\n",
    "        self.train = train\n",
    "        self.dataset_path = dataset_path\n",
    "        self.sample_size = sample_size\n",
    "        self.augment = augment\n",
    "        if self.train:\n",
    "            f = open(self.dataset_path + \"train.txt\", \"r\")\n",
    "        else:\n",
    "            f = open(self.dataset_path + \"val.txt\", \"r\")\n",
    "        self.index = f.read().splitlines()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "    \n",
    "    def theta_to_class(self, theta):\n",
    "        return int((theta + np.pi)/(np.pi/6))\n",
    "    \n",
    "    def theta_array_to_class(self, theta_array):\n",
    "        theta_class = np.array((theta_array + np.pi)/(np.pi/6),dtype=np.int)\n",
    "        theta_class[theta_class>=12] = 0\n",
    "        theta_class[theta_class<0] = 0\n",
    "        return theta_class\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx_str = self.index[idx]\n",
    "        scan = np.fromfile(self.dataset_path + \"velodyne/\" + idx_str + \".bin\", \n",
    "                            dtype=np.float32).reshape((-1,4))[:,0:3]\n",
    "        x = scan[:,0] + 0\n",
    "        y = scan[:,1] + 0\n",
    "        z = scan[:,2] + 0\n",
    "        mask = np.logical_and(np.logical_and(x>=0,x<=70.4),np.logical_and(y>=-40,y<=40),np.logical_and(z>=-3,z<=1))\n",
    "        x = x[mask]\n",
    "        y = y[mask]\n",
    "        z = z[mask]\n",
    "        scan = np.zeros((x.shape[0],3))\n",
    "        scan[:,0] = x\n",
    "        scan[:,1] = y\n",
    "        scan[:,2] = z\n",
    "        f = open(self.dataset_path + \"calib/\" + idx_str + \".txt\", \"r\")\n",
    "        lines = f.readlines()\n",
    "        R0 = np.array(lines[4].split(\" \")[1:], dtype=np.float).reshape((3,3))\n",
    "        Tr_veo_cam = np.array(lines[5].split(\" \")[1:], dtype=np.float).reshape((3,4))\n",
    "        Tr_cam_veo = inverse_rigid_trans(Tr_veo_cam)\n",
    "        f = open(self.dataset_path + \"label_2/\" + idx_str + \".txt\",\"r\")\n",
    "        lines = f.readlines()\n",
    "        labels = np.zeros((17,1 + 4 + 2 + 3 + 6 + 3 + 1)) # 1class, 3center, 1+1angle, 3+1size, 6xyz_min/max, \n",
    "        # 3mean_size, 1difficulty\n",
    "        for i in range(17): # padding some invalid labels\n",
    "            if i < len(lines):\n",
    "                data = lines[i].split(\" \")\n",
    "                if data[0] == \"Car\" or data[0] == \"Pedestrian\" or data[0] == \"Cyclist\":\n",
    "                    labels[i,4] = float(data[14])\n",
    "                    labels[i,5] = self.theta_to_class(labels[i,4])\n",
    "                    labels[i,7] = float(data[8])\n",
    "                    labels[i,8] = float(data[9])\n",
    "                    labels[i,9] = float(data[10])\n",
    "                    \n",
    "                    h = float(data[8])\n",
    "                    w = float(data[9])\n",
    "                    l = float(data[10])\n",
    "                    x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2]\n",
    "                    y_corners = [0, 0, 0, 0, -h, -h, -h, -h]\n",
    "                    z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2]\n",
    "                    R = np.zeros((3,3))\n",
    "                    a = np.cos(float(data[14]))\n",
    "                    b = np.sin(float(data[14]))\n",
    "                    R[0, 0] = a\n",
    "                    R[0, 2] = b\n",
    "                    R[1, 1] = 1.0\n",
    "                    R[2, 0] = -b\n",
    "                    R[2, 2] = a\n",
    "                    corners_3d = np.dot(R, np.vstack([x_corners, y_corners, z_corners]))\n",
    "                    corners_3d[0, :] = corners_3d[0, :] + float(data[11])\n",
    "                    corners_3d[1, :] = corners_3d[1, :] + float(data[12])\n",
    "                    corners_3d[2, :] = corners_3d[2, :] + float(data[13])\n",
    "                    corners_3d = np.transpose(np.dot(np.linalg.inv(R0), corners_3d)) # [8,3]\n",
    "                    n = corners_3d.shape[0]\n",
    "                    corners_3d_hom = np.hstack((corners_3d, np.ones((n, 1))))\n",
    "                    corners_3d = np.dot(corners_3d_hom, np.transpose(Tr_cam_veo))\n",
    "                    \n",
    "                    xmin = np.min(corners_3d[:,0])\n",
    "                    ymin = np.min(corners_3d[:,1])\n",
    "                    zmin = np.min(corners_3d[:,2])\n",
    "                    xmax = np.max(corners_3d[:,0])\n",
    "                    ymax = np.max(corners_3d[:,1])\n",
    "                    zmax = np.max(corners_3d[:,2])\n",
    "                    labels[i,10] = xmin\n",
    "                    labels[i,11] = xmax\n",
    "                    labels[i,12] = ymin\n",
    "                    labels[i,13] = ymax\n",
    "                    labels[i,14] = zmin\n",
    "                    labels[i,15] = zmax\n",
    "                    labels[i,1] = (xmin + xmax) / 2\n",
    "                    labels[i,2] = (ymin + ymax) / 2\n",
    "                    labels[i,3] = (zmin + zmax) / 2\n",
    "                                        \n",
    "                    labels[i,19] = int(data[2])\n",
    "                    if data[0] == \"Car\":\n",
    "                        labels[i, 0] = 1\n",
    "                    \n",
    "                        labels[i,6] = 0\n",
    "                    \n",
    "                        labels[i,16] = 1.5260834319114884\n",
    "                        labels[i,17] = 1.6285898684851439\n",
    "                        labels[i,18] = 3.883954491684643\n",
    "                    elif data[0] == \"Pedestrian\":\n",
    "                        labels[i, 0] = 2\n",
    "                    \n",
    "                        labels[i,6] = 1\n",
    "                    \n",
    "                        labels[i,16] = 1.7607064854022731\n",
    "                        labels[i,17] = 0.6601894361488745\n",
    "                        labels[i,18] = 0.8422843770893693\n",
    "                    else:\n",
    "                        labels[i, 0] = 3\n",
    "                    \n",
    "                        labels[i,6] = 2\n",
    "                    \n",
    "                        labels[i,16] = 1.73720344191764\n",
    "                        labels[i,17] = 0.5967732022126614\n",
    "                        labels[i,18] = 1.7635464044253226\n",
    "                else:\n",
    "                    for j in range(labels.shape[1]):\n",
    "                        labels[i,j] = -1000\n",
    "                    labels[i,0] = 0\n",
    "                    labels[i,5] = 0\n",
    "                    labels[i,6] = 0\n",
    "            else: # # 1class, 3center, 1_r+1_c angle, 1+3size, 6min maxcorners, 3mean_size, 1difficulty\n",
    "                for j in range(labels.shape[1]):\n",
    "                    labels[i,j] = -1000\n",
    "                labels[i,0] = 0\n",
    "                labels[i,5] = 0\n",
    "                labels[i,6] = 0\n",
    "\n",
    "        if self.augment:\n",
    "            # 1class, 3center, 1r+1cangle, 1+3size, 6min maxcorners, 3mean_size, 1difficulty\n",
    "            if np.random.random() > 0.5:\n",
    "                # Flipping along the YZ plane\n",
    "                scan[:,0] = -1 * scan[:,0]\n",
    "                labels[:,1] = -1 * labels[:,1]\n",
    "                x_min_origin = labels[:,10] + 0\n",
    "                x_max_origin = labels[:,11] + 0\n",
    "                labels[:,10] = -1 * x_max_origin\n",
    "                labels[:,11] = -1 * x_min_origin\n",
    "                theta_z = -labels[:,4] - np.pi/2 # -3/2pi ~ 1/2pi\n",
    "                theta_z[theta_z>=0] = np.pi - theta_z[theta_z>=0]\n",
    "                theta_z[theta_z<0] = -np.pi - theta_z[theta_z<0]\n",
    "                labels[:,4] = -theta_z - np.pi/2\n",
    "                labels[:,5] = self.theta_array_to_class(labels[:,4])\n",
    "\n",
    "\n",
    "            if np.random.random() > 0.5:\n",
    "                # Flipping along the XZ plane\n",
    "                scan[:,1] = -1 * scan[:,1]\n",
    "                labels[:,2] = -1 * labels[:,2]\n",
    "                y_min_origin = labels[:,12] + 0\n",
    "                y_max_origin = labels[:,13] + 0\n",
    "                labels[:,12] = -1 * y_max_origin\n",
    "                labels[:,13] = -1 * y_min_origin\n",
    "                theta_z = -labels[:,4] - np.pi/2 # -3/2pi ~ 1/2pi\n",
    "                theta_z = -theta_z\n",
    "                labels[:,4] = -theta_z - np.pi/2\n",
    "                labels[:,5] = self.theta_array_to_class(labels[:,4])\n",
    "\n",
    "            # Rotation along up-axis/Z-axis\n",
    "            theta = (np.random.random()*np.pi/18) - np.pi/36 # -5 ~ +5 degree\n",
    "            theta_ = labels[:,4] - theta\n",
    "            mask = np.zeros((theta_.shape[0],1))\n",
    "            mask[theta_>np.pi] = 1\n",
    "            mask[theta_<-np.pi] = 1\n",
    "            if np.sum(mask) > 0:\n",
    "                theta = 0\n",
    "            labels[:,4] -= theta\n",
    "            labels[:,5] = self.theta_array_to_class(labels[:,4])\n",
    "            matrix = np.zeros((3,3))\n",
    "            a = np.cos(theta)\n",
    "            b = np.sin(theta)\n",
    "            matrix[0, 0] = a\n",
    "            matrix[0, 1] = -b\n",
    "            matrix[1, 0] = b\n",
    "            matrix[1, 1] = a\n",
    "            matrix[2, 2] = 1.0\n",
    "            scan[:,0:3] = np.dot(scan[:,0:3], np.transpose(matrix))\n",
    "            labels[:,1:4] = np.dot(labels[:,1:4], np.transpose(matrix))\n",
    "            xyz_min = np.ones((labels.shape[0],3)) # 17,3 = padding\n",
    "            xyz_max = np.ones((labels.shape[0],3))\n",
    "            xyz_min[:,0] = labels[:,10]\n",
    "            xyz_max[:,0] = labels[:,11]\n",
    "            xyz_min[:,1] = labels[:,12]\n",
    "            xyz_max[:,1] = labels[:,13]\n",
    "            xyz_min[:,2] = labels[:,14]\n",
    "            xyz_max[:,2] = labels[:,15]\n",
    "            xyz_min[:,0:3] = np.dot(xyz_min[:,0:3], np.transpose(matrix))\n",
    "            xyz_max[:,0:3] = np.dot(xyz_max[:,0:3], np.transpose(matrix))\n",
    "            labels[:,10] = xyz_min[:,0] \n",
    "            labels[:,11] = xyz_max[:,0]\n",
    "            labels[:,12] = xyz_min[:,1]\n",
    "            labels[:,13] = xyz_max[:,1] \n",
    "            labels[:,14] = xyz_min[:,2]\n",
    "            labels[:,15] = xyz_max[:,2]\n",
    "            \n",
    "            # Rescale randomly by 0.9 - 1.1\n",
    "            proportion = np.random.uniform(0.9, 1.1, 1)\n",
    "            RATIO = proportion\n",
    "            scan[:,0:3] = scan[:,0:3] * proportion\n",
    "            labels[:,1:4] = labels[:,1:4] * proportion\n",
    "            labels[:,7:19] = labels[:,7:19] * proportion\n",
    "        else:\n",
    "            RATIO = 1\n",
    "        \n",
    "        sample_indexes = np.array(random.sample(range(0, scan.shape[0]), self.sample_size), dtype=np.long)\n",
    "        return torch.FloatTensor(scan), torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = KittiDataset(True, DATASET_PATH, SAMPLE_SIZE, True)\n",
    "val_set = KittiDataset(False, DATASET_PATH, SAMPLE_SIZE, False)\n",
    "training_dataloader = DataLoader(dataset = training_set, batch_size = BATCH_SIZE, \n",
    "                                 shuffle = True, drop_last = True, num_workers=4)\n",
    "val_dataloader = DataLoader(dataset = val_set, batch_size = BATCH_SIZE, \n",
    "                                 shuffle = False, drop_last = True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_dataloader))\n",
    "print(len(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_points(points, idx):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        points: input points data, [B, N, C]\n",
    "        idx: sample index data, [B, S]\n",
    "    Return:\n",
    "        new_points:, indexed points data, [B, S, C]\n",
    "    \"\"\"\n",
    "    device = points.device\n",
    "    B = points.shape[0]\n",
    "    view_shape = list(idx.shape)\n",
    "    view_shape[1:] = [1] * (len(view_shape) - 1)\n",
    "    repeat_shape = list(idx.shape)\n",
    "    repeat_shape[0] = 1\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device).view(view_shape).repeat(repeat_shape)\n",
    "    new_points = points[batch_indices, idx, :]\n",
    "    return new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def farthest_point_sample(xyz, num_centroids):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: pointcloud data, [B, N, 3]\n",
    "        num_centroids: number of samples(centroids)\n",
    "    Return:\n",
    "        centroids: sampled pointcloud index, [B, npoint]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    centroids = torch.zeros(B, num_centroids, dtype=torch.long).to(device)\n",
    "    distance = torch.ones(B, N).to(device) * 1e10\n",
    "    farthest = torch.randint(0, N, (B,), dtype=torch.long).to(device)\n",
    "    batch_indices = torch.arange(B, dtype=torch.long).to(device)\n",
    "    for i in range(num_centroids):\n",
    "        centroids[:, i] = farthest # index\n",
    "        centroid = xyz[batch_indices, farthest, :].view(B, 1, 3)\n",
    "        dist = torch.sum(torch.sqrt((xyz - centroid) ** 2), -1)\n",
    "        mask = dist < distance\n",
    "        distance[mask] = dist[mask]\n",
    "        farthest = torch.max(distance, -1)[1]\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_distance(src, dst):\n",
    "    \"\"\"\n",
    "    Calculate Euclid distance between each two points.\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zmï¼›\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "    Input:\n",
    "        src: source points, [B, N, C]\n",
    "        dst: target points, [B, M, C]\n",
    "    Output:\n",
    "        dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, _ = src.shape\n",
    "    _, M, _ = dst.shape\n",
    "    dist = -2 * torch.matmul(src, dst.permute(0, 2, 1))\n",
    "    dist += torch.sum(src ** 2, -1).view(B, N, 1)\n",
    "    dist += torch.sum(dst ** 2, -1).view(B, 1, M)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ball_point(radius, nsample, xyz, new_xyz):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        radius: local region radius\n",
    "        nsample: max sample number in local region\n",
    "        xyz: all points, [B, N, 3]\n",
    "        new_xyz: query points, [B, S, 3]\n",
    "    Return:\n",
    "        group_idx: grouped points index, [B, S, nsample]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    _, S, _ = new_xyz.shape\n",
    "    group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "    sqrdists = square_distance(new_xyz, xyz)\n",
    "    group_idx[sqrdists > radius ** 2] = N\n",
    "    group_idx = group_idx.sort(dim=-1)[0][:, :, :nsample]\n",
    "    group_first = group_idx[:, :, 0].view(B, S, 1).repeat([1, 1, nsample])\n",
    "    mask = group_idx == N\n",
    "    group_idx[mask] = group_first[mask]\n",
    "    return group_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_group(xyz, feature, num_centroids, num_neighbors, radius):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        xyz: input points position data, [B, N, 3]\n",
    "        feature: input feature data, [B, N, D]\n",
    "        num_centroids:\n",
    "        num_neighbors:\n",
    "        radius:\n",
    "    Return:\n",
    "        centroids: sampled points position data, [B, num_centroids, num_neighbors, 3]\n",
    "        new_points: sampled position+feature data, [B, num_centroids, num_neighbors, 3 + D]\n",
    "    \"\"\"\n",
    "    device = xyz.device\n",
    "    B, N, C = xyz.shape\n",
    "    S = num_centroids\n",
    "    # get centroids\n",
    "    centroids_index = farthest_point_sample(xyz, num_centroids) # [B, num_centroids]\n",
    "    centroids = index_points(xyz, centroids_index)\n",
    "    neighbors_index = query_ball_point(radius, num_neighbors, xyz, centroids)\n",
    "    neighbors = index_points(xyz, neighbors_index) # [B, npoint, nsample, C]\n",
    "    neighbors_norm = neighbors - centroids.view(B, S, 1, C)\n",
    "    \n",
    "    if feature is not None:\n",
    "        feature_neighbors = index_points(feature, neighbors_index)\n",
    "        new_points = torch.cat([neighbors_norm, feature_neighbors], dim = -1)\n",
    "    else:\n",
    "        new_points = neighbors_norm\n",
    "    \n",
    "    return centroids, new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetSetAbstraction(nn.Module):\n",
    "    def __init__(self, num_centroids, radius, num_neighbors, in_channel, mlp):\n",
    "        super(PointNetSetAbstraction, self).__init__()\n",
    "        self.num_centroids = num_centroids\n",
    "        self.radius = radius\n",
    "        self.num_neighbors = num_neighbors\n",
    "        \n",
    "        self.conv_list = nn.ModuleList()\n",
    "        self.bn_list = nn.ModuleList()\n",
    "        \n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.conv_list.append(nn.Conv2d(last_channel, out_channel, 1, 1))\n",
    "            self.bn_list.append(nn.BatchNorm2d(out_channel))\n",
    "            last_channel = out_channel\n",
    "        \n",
    "    def forward(self, xyz, feature):\n",
    "        # xyz: [B, C, N] feature: [B, D, N]\n",
    "        xyz = xyz.permute(0, 2, 1)\n",
    "        if feature is not None:\n",
    "            feature = feature.permute(0, 2, 1)\n",
    "        \n",
    "        centroids, new_points = sample_and_group(xyz, feature, self.num_centroids, self.num_neighbors, self.radius)\n",
    "        # new_points [B, num_centroids, num_neighbors, 3 + D]\n",
    "        new_points = new_points.permute(0, 3, 2, 1) # [B, 3 + D, num_neighbors, num_centroids]\n",
    "        for i, conv in enumerate(self.conv_list):\n",
    "            bn = self.bn_list[i]\n",
    "            new_points = F.relu(bn(conv(new_points)), inplace=True)\n",
    "        new_points = torch.max(new_points, 2)[0] # dim = 2 -> reduce the third dimension = num_neighbors\n",
    "        new_xyz = centroids.permute(0, 2, 1) # from [B, N, C] to [B, C, N]\n",
    "        return new_xyz, new_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetFeaturePropagation(nn.Module):\n",
    "    def __init__(self, in_channel, mlp):\n",
    "        super(PointNetFeaturePropagation, self).__init__()\n",
    "        self.conv_list = nn.ModuleList()\n",
    "        self.bn_list = nn.ModuleList()\n",
    "        last_channel = in_channel\n",
    "        for out_channel in mlp:\n",
    "            self.conv_list.append(nn.Conv1d(last_channel, out_channel, 1))\n",
    "            self.bn_list.append(nn.BatchNorm1d(out_channel))\n",
    "            last_channel = out_channel\n",
    "    \n",
    "    def forward(self, xyz1, xyz2, feature1, feature2):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            xyz1: input points position data, [B, C, N]\n",
    "            xyz2: sampled input points position data, [B, C, S]\n",
    "            feature1: input feature data, [B, D, N]\n",
    "            feature2: input feature data, [B, D, S]\n",
    "        Return:\n",
    "            new_feature: upsampled feature data, [B, D', N]\n",
    "        \"\"\"\n",
    "        xyz1 = xyz1.permute(0, 2, 1) # [B, N, C]\n",
    "        xyz2 = xyz2.permute(0, 2, 1) # [B, S, C]\n",
    "        \n",
    "        B = xyz1.shape[0]\n",
    "        N = xyz1.shape[1]\n",
    "        S = xyz2.shape[1]\n",
    "        D = feature2.shape[1]\n",
    "        \n",
    "        dists = square_distance(xyz1, xyz2)\n",
    "        dists, idx = dists.sort(dim = -1) # [B, N, S]\n",
    "        dists, idx = dists[:, :, :3], idx[:, :, :3]  # [B, N, 3]\n",
    "        \n",
    "        dist_rev = 1.0 / (dists + 1e-8) # [B, N, 3]\n",
    "        norm = torch.sum(dist_rev, dim = 2, keepdim = True) # [B, N, 3]\n",
    "        weights = dist_rev / norm # [B, N, 3]\n",
    "        \n",
    "        feature2 = feature2.permute(0, 2, 1) # [B, S, D]\n",
    "        interpolated_feature = torch.sum(index_points(feature2, idx) * weights.view(B, N, 3, 1), dim=2)\n",
    "        \n",
    "        if feature1 is None:\n",
    "            new_feature = interpolated_feature\n",
    "        else:\n",
    "            feature1 = feature1.permute(0, 2, 1) # [B, N, D]\n",
    "            new_feature = torch.cat([feature1, interpolated_feature], dim = -1) # [B, N, D + ?]\n",
    "        \n",
    "        new_feature = new_feature.permute(0, 2, 1) #[B, D + ?, N]\n",
    "        \n",
    "        for i, conv in enumerate(self.conv_list):\n",
    "            bn = self.bn_list[i]\n",
    "            new_feature = F.relu(bn(conv(new_feature)), inplace=True)\n",
    "        return new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointNet2, self).__init__()\n",
    "        self.sa1 = PointNetSetAbstraction(1024, 2, 64, 3, [32, 32, 64])\n",
    "        self.sa2 = PointNetSetAbstraction(512, 4, 64, 64 + 3, [64, 64, 128])\n",
    "        self.sa3 = PointNetSetAbstraction(256, 6, 64, 128 + 3, [128, 128, 256])\n",
    "        self.sa4 = PointNetSetAbstraction(128, 8, 64, 256 + 3, [256, 256, 512])\n",
    "        self.fp4 = PointNetFeaturePropagation(256 + 512, [256, 256])\n",
    "        self.fp3 = PointNetFeaturePropagation(128 + 256, [256, 256])\n",
    "        self.fp2 = PointNetFeaturePropagation(64 + 256, [256, 256])\n",
    "        #self.fp1 = PointNetFeaturePropagation(256, [256, 256])\n",
    "        \n",
    "    def forward(self, xyz):\n",
    "        # xyz: [B, 3, N]\n",
    "        xyz_0 = xyz[:,0:3,:]\n",
    "        feature_0 = None\n",
    "        B, N = xyz.shape[0], xyz.shape[2]\n",
    "        \n",
    "        xyz_1, feature_1 = self.sa1(xyz_0, feature_0)\n",
    "        xyz_2, feature_2 = self.sa2(xyz_1, feature_1)\n",
    "        xyz_3, feature_3 = self.sa3(xyz_2, feature_2)\n",
    "        xyz_4, feature_4 = self.sa4(xyz_3, feature_3)\n",
    "        \n",
    "        feature_3 = self.fp4(xyz_3, xyz_4, feature_3, feature_4)\n",
    "        feature_2 = self.fp3(xyz_2, xyz_3, feature_2, feature_3)\n",
    "        feature_1 = self.fp2(xyz_1, xyz_2, feature_1, feature_2)\n",
    "        #feature_0 = self.fp1(xyz_0, xyz_1, feature_0, feature_1)\n",
    "        \n",
    "        return xyz_1, feature_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vote(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vote, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(256, 256, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        #self.conv1_1 = nn.Conv1d(256, 256, 1)\n",
    "        self.conv2 = nn.Conv1d(256, 256, 1)\n",
    "        #self.conv2_1 = nn.Conv1d(256, 256, 1)\n",
    "        #self.conv2_2 = nn.Conv1d(256, 256, 1)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.conv3 = nn.Conv1d(256, 259, 1)\n",
    "    \n",
    "    def forward(self, feature):\n",
    "        vote = F.relu(self.bn1(self.conv1(feature)), inplace=True)\n",
    "        vote = F.relu(self.bn2(self.conv2(vote)), inplace=True)\n",
    "        vote = self.conv3(vote)\n",
    "        return vote[:,0:3,:], vote[:,3:,:] # xyz offset and feature residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalModule(nn.Module):\n",
    "    def __init__(self, num_proposals, radius, num_neighbors, vote_dimension, mlp, num_classes):\n",
    "        super(ProposalModule, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.vote_aggregation = PointNetSetAbstraction(num_proposals, \n",
    "                                                       radius, num_neighbors, \n",
    "                                                       vote_dimension, mlp)\n",
    "        self.conv1 = nn.Conv1d(128, 128, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.conv3 = nn.Conv1d(128, 2 + 4 + 3 + 2 * 12 + 4 * 3, 1) # objectness, class, center, angle, size\n",
    "        \n",
    "    def forward(self, xyz, feature):\n",
    "        xyz, new_points = self.vote_aggregation(xyz, feature) # new_points 3+D, feature 3+D' [B, num_proposals, 3 + D]\n",
    "        proposal = F.relu(self.bn1(self.conv1(new_points)), inplace=True)\n",
    "        proposal = F.relu(self.bn2(self.conv2(proposal)), inplace=True)\n",
    "        proposal = self.conv3(proposal)\n",
    "        return xyz, proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoteNet(nn.Module):\n",
    "    def __init__(self, num_proposals, radius, num_neighbors, vote_dimension, mlp, num_classes):\n",
    "        super(VoteNet, self).__init__()\n",
    "        self.backbone = PointNet2()\n",
    "        self.vote = Vote()\n",
    "        # num_centroids, radius, num_neighbors, in_channel, mlp\n",
    "        self.proposal = ProposalModule(num_proposals, radius, num_neighbors, vote_dimension, mlp, num_classes)\n",
    "    \n",
    "    def forward(self, xyz):\n",
    "        seed, feature = self.backbone(xyz)\n",
    "        xyz_offset, feature_residual = self.vote(feature)\n",
    "        vote = seed + xyz_offset\n",
    "        feature = feature + feature_residual\n",
    "        vote, proposals = self.proposal(vote, feature)\n",
    "        return seed, xyz_offset, vote, proposals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(error, delta=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        error: Torch tensor (d1,d2,...,dk)\n",
    "    Returns:\n",
    "        loss: Torch tensor (d1,d2,...,dk)\n",
    "    x = error = pred - gt or dist(pred,gt)\n",
    "    0.5 * |x|^2                 if |x|<=d\n",
    "    0.5 * d^2 + d * (|x|-d)     if |x|>d\n",
    "    Ref: https://github.com/charlesq34/frustum-pointnets/blob/master/models/model_util.py\n",
    "    \"\"\"\n",
    "    abs_error = torch.abs(error)\n",
    "    #quadratic = torch.min(abs_error, torch.FloatTensor([delta]))\n",
    "    quadratic = torch.clamp(abs_error, max=delta)\n",
    "    linear = (abs_error - quadratic)\n",
    "    loss = 0.5 * quadratic**2 + delta * linear\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_distance(pc1, pc2, l1smooth=False, delta=1.0, l1=False):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        pc1: (B,N,C) torch tensor\n",
    "        pc2: (B,M,C) torch tensor\n",
    "        l1smooth: bool, whether to use l1smooth loss\n",
    "        delta: scalar, the delta used in l1smooth loss\n",
    "    Output:\n",
    "        dist1: (B,N) torch float32 tensor\n",
    "        idx1: (B,N) torch int64 tensor\n",
    "        dist2: (B,M) torch float32 tensor\n",
    "        idx2: (B,M) torch int64 tensor\n",
    "    \"\"\"\n",
    "    N = pc1.shape[1]\n",
    "    M = pc2.shape[1]\n",
    "    pc1_expand_tile = pc1.unsqueeze(2).repeat(1,1,M,1)\n",
    "    pc2_expand_tile = pc2.unsqueeze(1).repeat(1,N,1,1)\n",
    "    pc_diff = pc1_expand_tile - pc2_expand_tile\n",
    "    \n",
    "    if l1smooth:\n",
    "        pc_dist = torch.sum(huber_loss(pc_diff, delta), dim=-1) # (B,N,M)\n",
    "    elif l1:\n",
    "        pc_dist = torch.sum(torch.abs(pc_diff), dim=-1) # (B,N,M)\n",
    "    else:\n",
    "        pc_dist = torch.sum(pc_diff**2, dim=-1) # (B,N,M)\n",
    "    dist1, idx1 = torch.min(pc_dist, dim=2) # (B,N)\n",
    "    dist2, idx2 = torch.min(pc_dist, dim=1) # (B,M)\n",
    "    return dist1, idx1, dist2, idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = \"{0:%Y-%m-%dT%H-%M-%S/}\".format(datetime.now())\n",
    "\n",
    "OBJECT_LIST = []\n",
    "MASK_LIST = []\n",
    "VOTE_LIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, ignore_index=0, objectness_weight=None, semantic_weight=None, weight=None):\n",
    "        super(Loss, self).__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.objectness_weight = objectness_weight\n",
    "        self.semantic_weight = semantic_weight\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, seed, xyz_offset, vote, proposals, label):\n",
    "        # proposals -> # 2objectness, 4class, 3center, 12*2angle, 3*4size\n",
    "        # label -> # 1class, 3center, 1r+1cangle, 1+3size, 6min maxcorners, 3mean_size, 1difficulty\n",
    "        B = seed.shape[0]\n",
    "        K = proposals.shape[2]\n",
    "        semantic_label = label[:,:,0].long()\n",
    "        center = label[:,:,1:4]\n",
    "        angle_reg = label[:,:,4]\n",
    "        angle_cls = label[:,:,5].long()\n",
    "        angle_residual = angle_reg - (-np.pi + np.pi / 12 + (angle_cls.float()+1) * (np.pi / 6))\n",
    "        size_cls = label[:,:,6].long()\n",
    "        size_reg = label[:,:,7:10] # height, width, length\n",
    "        mean_size = label[:,:,16:19]\n",
    "        size_residual = size_reg - mean_size\n",
    "        min_max = label[:,:,10:16] # xyz_min/max\n",
    "        \n",
    "        difficulty = label[:,:,19]\n",
    "        \n",
    "        predict_objectness = proposals[:,0:2,:]\n",
    "        predict_semantic_label = proposals[:,2:6,:]\n",
    "        predict_center = proposals[:,6:9,:]\n",
    "        predict_angle_cls = proposals[:,21:33,:]\n",
    "        predict_angle_residual = proposals[:,9:21,:]\n",
    "        predict_size_cls = proposals[:,33:36,:]\n",
    "        predict_size_residual = proposals[:,36:45]\n",
    "        \n",
    "        # vote loss\n",
    "        # for seed if it is in one of the boxes -> calculate vote loss\n",
    "        xmin = min_max[:,:,0]\n",
    "        xmax = min_max[:,:,1]\n",
    "        ymin = min_max[:,:,2]\n",
    "        ymax = min_max[:,:,3]\n",
    "        zmin = min_max[:,:,4]\n",
    "        zmax = min_max[:,:,5]\n",
    "        \n",
    "        counter = 0\n",
    "        vote_criterion = nn.L1Loss()\n",
    "        vote_loss = None\n",
    "        isVote = False\n",
    "        for i in range(seed.shape[0]):\n",
    "            for j in range(seed.shape[2]):\n",
    "                for k in range(label.shape[1]):\n",
    "                    if semantic_label[i,k]>0 and seed[i,0,j]<=xmax[i,k] and seed[i,0,j]>=xmin[i,k] and seed[i,1,j]<=ymax[i,k] and seed[i,1,j]>=ymin[i,k] and seed[i,2,j]<=zmax[i,k] and seed[i,2,j]>=zmin[i,k]:\n",
    "                        counter += 1\n",
    "                        isVote = True\n",
    "                        true_offset = center.permute(0,2,1)[i,:,k] - seed[i,:,j]\n",
    "                        if counter == 1:\n",
    "                            vote_loss = self.weight[difficulty[i,k].long(), semantic_label[i,k]] * vote_criterion(xyz_offset[i,:,j], true_offset)\n",
    "                        else:\n",
    "                            vote_loss += self.weight[difficulty[i,k].long(), semantic_label[i,k]] * vote_criterion(xyz_offset[i,:,j], true_offset)\n",
    "        if isVote:\n",
    "            vote_loss /= counter\n",
    "        VOTE_LIST.append(counter)\n",
    "        \n",
    "        # objectness loss\n",
    "        objectness_criterion = nn.CrossEntropyLoss(weight=self.objectness_weight, reduction=\"none\")\n",
    "        objectness_label = torch.zeros((B, K), dtype=torch.long).to(device) #[B, K]\n",
    "        object_mask = torch.zeros((B, K)).to(device)\n",
    "        dis1, ind1, _, _ = nn_distance(vote.permute(0,2,1), center) # ind1 -> [B, K], K = num_proposals\n",
    "        objectness_label[torch.sqrt(dis1+1e-6)<NEAR_THRESHOLD] = 1\n",
    "        OBJECT_LIST.append(torch.sum(objectness_label))\n",
    "        object_mask[torch.sqrt(dis1+1e-6)<NEAR_THRESHOLD] = 1\n",
    "        object_mask[torch.sqrt(dis1+1e-6)>FAR_THRESHOLD] = 1\n",
    "        MASK_LIST.append(torch.sum(object_mask))\n",
    "        objectness_loss = objectness_criterion(predict_objectness, objectness_label)\n",
    "        objectness_loss = torch.sum(objectness_loss*object_mask)/(torch.sum(object_mask)+1e-6)\n",
    "        object_assignment = ind1\n",
    "        \n",
    "        if torch.sum(objectness_label) > 0 and counter > 0:\n",
    "            # semantic loss\n",
    "            semantic_criterion = nn.CrossEntropyLoss(ignore_index=0, reduction=\"none\")\n",
    "            semantic_label = torch.gather(semantic_label, 1, object_assignment) # select (B,K) from (B,K2)\n",
    "            difficulty_label = torch.gather(difficulty, 1, object_assignment)\n",
    "            weight_mask = torch.zeros((B,K)).to(device)\n",
    "            weight_mask[semantic_label==1] = self.semantic_weight[1] \n",
    "            weight_mask[semantic_label==2] = self.semantic_weight[2]\n",
    "            weight_mask[semantic_label==3] = self.semantic_weight[3]\n",
    "            weight_mask[difficulty_label==1] *= 1\n",
    "            weight_mask[difficulty_label==2] *= 1\n",
    "            semantic_loss = semantic_criterion(predict_semantic_label, semantic_label) # (B,K)\n",
    "            semantic_loss = torch.sum(semantic_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "        \n",
    "            # center loss\n",
    "            dist1, ind1, _, _ = nn_distance(predict_center.permute(0,2,1), center, l1smooth=True) # dist1: BxK\n",
    "            center_loss = torch.sum(dist1*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "        \n",
    "            # angle loss\n",
    "            angle_cls = torch.gather(angle_cls, 1, object_assignment) # select (B,K) from (B,K2)\n",
    "            angle_cls_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "            angle_cls_loss = angle_cls_criterion(predict_angle_cls, angle_cls) # (B,K)\n",
    "            angle_cls_loss = torch.sum(angle_cls_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "            \n",
    "            angle_residual = torch.gather(angle_residual, 1, object_assignment) # select (B,K) from (B,K2)\n",
    "            angle_residual_normalized = angle_residual / (np.pi / 12)\n",
    "            angle_one_hot = torch.FloatTensor(B, angle_cls.shape[1], 12).zero_().to(device)\n",
    "            angle_one_hot.scatter_(2, angle_cls.unsqueeze(-1), 1) # src==1 so it's *one-hot* (B,K,num_heading_bin)\n",
    "            angle_residual_normalized_loss = huber_loss(torch.sum(predict_angle_residual.permute(0,2,1)*angle_one_hot, -1) - angle_residual_normalized, delta=1.0) # (B,K)\n",
    "            angle_residual_normalized_loss = torch.sum(angle_residual_normalized_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "        \n",
    "            # size loss\n",
    "            size_cls = torch.gather(size_cls, 1, object_assignment) # select (B,K) from (B,K2)\n",
    "            size_cls_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "            size_cls_loss = size_cls_criterion(predict_size_cls, size_cls) # (B,K)\n",
    "            size_cls_loss = torch.sum(size_cls_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "        \n",
    "            size_residual = torch.gather(size_residual, 1, object_assignment.unsqueeze(-1).repeat(1,1,3)) # select (B,K,3) from (B,K2,3)\n",
    "            size_one_hot = torch.FloatTensor(B, size_cls.shape[1], 3).zero_().to(device)\n",
    "            size_one_hot.scatter_(2, size_cls.unsqueeze(-1), 1) # src==1 so it's *one-hot* (B,K,num_size_cluster)\n",
    "            size_one_hot_tiled = size_one_hot.unsqueeze(-1).repeat(1,1,1,3) # (B,K,num_size_cluster,3)\n",
    "            predicted_size_residual_normalized = torch.sum(predict_size_residual.view(B,K,3,3)*size_one_hot_tiled, 2) # (B,K,3)\n",
    "        \n",
    "            mean_size_arr_expanded = MEAN_SIZE_ARR.unsqueeze(0).unsqueeze(0) * RATIO # (1,1,num_size_cluster,3)\n",
    "            mean_size_label = torch.sum(size_one_hot_tiled * mean_size_arr_expanded, 2) # (B,K,3)\n",
    "            size_residual_normalized = size_residual / mean_size_label # (B,K,3)\n",
    "            size_residual_normalized_loss = torch.mean(huber_loss(predicted_size_residual_normalized - size_residual_normalized, delta=1.0), -1) # (B,K,3) -> (B,K)\n",
    "            size_residual_normalized_loss = torch.sum(size_residual_normalized_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "\n",
    "            box_loss = center_loss + 0.1*angle_cls_loss + angle_residual_normalized_loss + 0.3*size_cls_loss + size_residual_normalized_loss\n",
    "            return box_loss + 0.5 * objectness_loss + vote_loss + semantic_loss\n",
    "        elif torch.sum(objectness_label) == 0 and counter > 0:\n",
    "            return vote_loss + 0.5 * objectness_loss\n",
    "        elif torch.sum(objectness_label) > 0 and counter == 0:\n",
    "            # semantic_loss\n",
    "            semantic_criterion = nn.CrossEntropyLoss(ignore_index=0, reduction=\"none\")\n",
    "            semantic_label = torch.gather(semantic_label, 1, object_assignment) # select (B,K) from (B,K2)\n",
    "            difficulty_label = torch.gather(difficulty, 1, object_assignment)\n",
    "            weight_mask = torch.zeros((B,K)).to(device)\n",
    "            weight_mask[semantic_label==1] = self.semantic_weight[1] \n",
    "            weight_mask[semantic_label==2] = self.semantic_weight[2]\n",
    "            weight_mask[semantic_label==3] = self.semantic_weight[3]\n",
    "            weight_mask[difficulty_label==1] *= 1\n",
    "            weight_mask[difficulty_label==2] *= 1\n",
    "            semantic_loss = semantic_criterion(predict_semantic_label, semantic_label) # (B,K)\n",
    "            semantic_loss = torch.sum(semantic_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "        \n",
    "            # center loss\n",
    "            dist1, ind1, _, _ = nn_distance(predict_center.permute(0,2,1), center, l1smooth=True) # dist1: BxK\n",
    "            center_loss = torch.sum(dist1*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "        \n",
    "            # angle loss\n",
    "            angle_cls = torch.gather(angle_cls, 1, object_assignment) # select (B,K) from (B,K2)\n",
    "            angle_cls_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "            angle_cls_loss = angle_cls_criterion(predict_angle_cls, angle_cls) # (B,K)\n",
    "            angle_cls_loss = torch.sum(angle_cls_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "            \n",
    "            angle_residual = torch.gather(angle_residual, 1, object_assignment) # select (B,K) from (B,K2)\n",
    "            angle_residual_normalized = angle_residual / (np.pi / 12)\n",
    "            angle_one_hot = torch.FloatTensor(B, angle_cls.shape[1], 12).zero_().to(device)\n",
    "            angle_one_hot.scatter_(2, angle_cls.unsqueeze(-1), 1) # src==1 so it's *one-hot* (B,K,num_heading_bin)\n",
    "            angle_residual_normalized_loss = huber_loss(torch.sum(predict_angle_residual.permute(0,2,1)*angle_one_hot, -1) - angle_residual_normalized, delta=1.0) # (B,K)\n",
    "            angle_residual_normalized_loss = torch.sum(angle_residual_normalized_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "        \n",
    "            # size loss\n",
    "            size_cls = torch.gather(size_cls, 1, object_assignment) # select (B,K) from (B,K2)\n",
    "            size_cls_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "            size_cls_loss = size_cls_criterion(predict_size_cls, size_cls) # (B,K)\n",
    "            size_cls_loss = torch.sum(size_cls_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "        \n",
    "            size_residual = torch.gather(size_residual, 1, object_assignment.unsqueeze(-1).repeat(1,1,3)) # select (B,K,3) from (B,K2,3)\n",
    "            size_one_hot = torch.FloatTensor(B, size_cls.shape[1], 3).zero_().to(device)\n",
    "            size_one_hot.scatter_(2, size_cls.unsqueeze(-1), 1) # src==1 so it's *one-hot* (B,K,num_size_cluster)\n",
    "            size_one_hot_tiled = size_one_hot.unsqueeze(-1).repeat(1,1,1,3) # (B,K,num_size_cluster,3)\n",
    "            predicted_size_residual_normalized = torch.sum(predict_size_residual.view(B,K,3,3)*size_one_hot_tiled, 2) # (B,K,3)\n",
    "        \n",
    "            mean_size_arr_expanded = MEAN_SIZE_ARR.unsqueeze(0).unsqueeze(0) * RATIO # (1,1,num_size_cluster,3)\n",
    "            mean_size_label = torch.sum(size_one_hot_tiled * mean_size_arr_expanded, 2) # (B,K,3)\n",
    "            size_residual_normalized = size_residual / mean_size_label # (B,K,3)\n",
    "            size_residual_normalized_loss = torch.mean(huber_loss(predicted_size_residual_normalized - size_residual_normalized, delta=1.0), -1) # (B,K,3) -> (B,K)\n",
    "            size_residual_normalized_loss = torch.sum(size_residual_normalized_loss*(objectness_label.float())*weight_mask)/(torch.sum(objectness_label)+1e-6)\n",
    "\n",
    "            box_loss = center_loss + 0.1*angle_cls_loss + angle_residual_normalized_loss + 0.3*size_cls_loss + size_residual_normalized_loss\n",
    "            return 0.5 * objectness_loss + box_loss + semantic_loss\n",
    "        else:\n",
    "            return 0.5 * objectness_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proportion = torch.FloatTensor([1e-6,28742,4487,1627])\n",
    "#semantic_weight = torch.sqrt(1 / (proportion / torch.sum(proportion)))\n",
    "#semantic_weight[0] = 1e-6\n",
    "semantic_weight = torch.FloatTensor([1e-6,1,1,1])\n",
    "print(semantic_weight)\n",
    "weight = torch.zeros((4,4)).to(device)\n",
    "weight[0:4,:] = semantic_weight\n",
    "#weight[1,:] *= 2\n",
    "#weight[2,:] *= 3\n",
    "print(weight)\n",
    "#objectness_proportion = torch.FloatTensor([501.64367816091954, 2.8735632183908044])\n",
    "#objectness_weight = torch.log(1 / (objectness_proportion / torch.sum(objectness_proportion))).to(device)\n",
    "objectness_weight = torch.FloatTensor([0.05, 0.95]).to(device)\n",
    "print(objectness_weight)\n",
    "votenet = VoteNet(num_proposals=512, radius=1, num_neighbors=64, vote_dimension=259, mlp=[128,128,128], num_classes=4)\n",
    "criterion = Loss(ignore_index=0, objectness_weight=objectness_weight, semantic_weight=semantic_weight, weight=weight)\n",
    "optimizer = torch.optim.Adam(votenet.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in votenet.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Conv1d)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in votenet.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"runs/loss\" + TIMESTAMP)\n",
    "\n",
    "votenet.to(device)\n",
    "\n",
    "for epoch in range(50):\n",
    "    if (epoch+1) % 6 == 0:# every 5 epoch update\n",
    "        lr_scheduler.step()\n",
    "    train_running_loss = 0.0\n",
    "    val_running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    break_signal= False\n",
    "    for i, data in enumerate(training_dataloader, 0):\n",
    "        votenet.train()\n",
    "        X, y = data\n",
    "        X = X.permute(0, 2, 1)\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        seed, xyz_offset, vote, proposals = votenet(X)\n",
    "        loss = criterion(seed, xyz_offset, vote, proposals, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_running_loss += loss.item()\n",
    "        \n",
    "        if i % 20 == 19:\n",
    "            with torch.no_grad():\n",
    "                votenet.eval()\n",
    "                random_indexes = np.array(random.sample(range(0, len(val_dataloader)), 20), dtype=np.long)\n",
    "                for j in range(len(random_indexes)):\n",
    "                    X_val, y_val = val_set[random_indexes[j]]\n",
    "                    X_val = X_val.view(-1, SAMPLE_SIZE, 3).permute(0, 2, 1)\n",
    "                    y_val = y_val.view(-1, 17, 20)\n",
    "                    X_val = X_val.to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    seed, xyz_offset, vote, proposals = votenet(X_val)\n",
    "                    val_running_loss += criterion(seed, xyz_offset, vote, proposals, y_val).item()\n",
    "                        \n",
    "            train_running_loss /= 20\n",
    "            val_running_loss /= 20\n",
    "        \n",
    "            with open('loss.txt','a') as f:\n",
    "                f.write(\"[Epoch %d, Iteration %5d] train_loss: %.3f acc: %.2f %% val_loss: %.3f\\n\" % \n",
    "                        (epoch+1, i+1, train_running_loss, 100*correct, val_running_loss))\n",
    "        \n",
    "            writer.add_scalars('loss', {'training_loss':train_running_loss,\n",
    "                                        'val_loss':val_running_loss}, epoch * len(training_dataloader) + i)\n",
    "\n",
    "            train_running_loss = 0.0\n",
    "            val_running_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0\n",
    "            writer.flush()\n",
    "    if break_signal:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECT_LIST = np.array(OBJECT_LIST, dtype=np.float)\n",
    "MASK_LIST = np.array(MASK_LIST, dtype=np.float)\n",
    "VOTE_LIST = np.array(VOTE_LIST, dtype=np.float)\n",
    "print(np.mean(OBJECT_LIST))\n",
    "print(np.mean(MASK_LIST))\n",
    "print(np.mean(OBJECT_LIST) / np.mean(MASK_LIST))\n",
    "print(np.mean(VOTE_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votenet.eval()\n",
    "torch.save(votenet.state_dict(), \"votenet_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box3d_vol(corners):\n",
    "    ''' corners: (8,3) no assumption on axis direction '''\n",
    "    a = np.sqrt(np.sum((corners[0,:] - corners[1,:])**2))\n",
    "    b = np.sqrt(np.sum((corners[1,:] - corners[2,:])**2))\n",
    "    c = np.sqrt(np.sum((corners[0,:] - corners[4,:])**2))\n",
    "    return a*b*c\n",
    "\n",
    "def poly_area(x,y):\n",
    "    \"\"\" Ref: http://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates \"\"\"\n",
    "    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\n",
    "\n",
    "def convex_hull_intersection(p1, p2):\n",
    "    \"\"\" Compute area of two convex hull's intersection area.\n",
    "        p1,p2 are a list of (x,y) tuples of hull vertices.\n",
    "        return a list of (x,y) for the intersection and its volume\n",
    "    \"\"\"\n",
    "    poly1 = Polygon(p1).convex_hull\n",
    "    poly2 = Polygon(p2).convex_hull\n",
    "\n",
    "    if not poly1.intersects(poly2):\n",
    "        inter_area = 0\n",
    "    else:\n",
    "        inter_area = poly1.intersection(poly2).area\n",
    "    return inter_area\n",
    "\n",
    "def bboxIoU(corners1, corners2):\n",
    "    # corner points are in counter clockwise order\n",
    "    # up direction is negative Y\n",
    "    temp_z1 = -corners1[:,2]\n",
    "    temp_z2 = -corners2[:,2]\n",
    "    corners1[:,2] = corners1[:,1]\n",
    "    corners2[:,2] = corners2[:,1]\n",
    "    corners1[:,1] = temp_z1\n",
    "    corners2[:,1] = temp_z2\n",
    "    rect1 = [(corners1[i,0], corners1[i,2]) for i in range(3,-1,-1)]\n",
    "    rect2 = [(corners2[i,0], corners2[i,2]) for i in range(3,-1,-1)]\n",
    "    \n",
    "    area1 = poly_area(np.array(rect1)[:,0], np.array(rect1)[:,1])\n",
    "    area2 = poly_area(np.array(rect2)[:,0], np.array(rect2)[:,1])\n",
    "   \n",
    "    inter_area = convex_hull_intersection(rect1, rect2)\n",
    "    iou_2d = inter_area/(area1+area2-inter_area)\n",
    "    ymax = min(corners1[0,1], corners2[0,1])\n",
    "    ymin = max(corners1[4,1], corners2[4,1])\n",
    "\n",
    "    inter_vol = inter_area * max(0.0, ymax-ymin)\n",
    "    \n",
    "    vol1 = box3d_vol(corners1)\n",
    "    vol2 = box3d_vol(corners2)\n",
    "    iou = inter_vol / (vol1 + vol2 - inter_vol)\n",
    "    return iou, iou_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bb_box(center, angle, size):\n",
    "    if angle.shape[0] == 24: # predict\n",
    "        angle_cls = torch.max(F.softmax(angle[12:,],dim=0),dim=0)[1]\n",
    "        # un - normalized\n",
    "        angle_residual = angle[angle_cls] * (np.pi/12)\n",
    "        size_cls = torch.max(F.softmax(size[0:3]), dim=0)[1]\n",
    "        # un - normalized\n",
    "        size_residual = size[3+size_cls*3:3+size_cls*3+3] * MEAN_SIZE_ARR[size_cls]\n",
    "        size = MEAN_SIZE_ARR[size_cls] + size_residual\n",
    "        angle = -np.pi + np.pi/12 + angle_cls * (np.pi/6) + angle_residual\n",
    "        \n",
    "    elif angle.shape[0] != 24: # ground truth labels\n",
    "        angle = angle[0]\n",
    "        size = size[1:4]\n",
    "        \n",
    "    angle = -np.pi/2 - angle # from camera y to lidar z\n",
    "    h = size[0].cpu().detach().numpy()\n",
    "    w = size[1].cpu().detach().numpy()\n",
    "    l = size[2].cpu().detach().numpy()\n",
    "    x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2]\n",
    "    y_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2]\n",
    "    z_corners = [0, 0, 0, 0, h, h, h, h]\n",
    "    matrix = np.zeros((3,3))\n",
    "    a = np.cos(angle.cpu().detach().numpy())\n",
    "    b = np.sin(angle.cpu().detach().numpy())\n",
    "    matrix[0, 0] = a\n",
    "    matrix[0, 1] = -b\n",
    "    matrix[1, 0] = b\n",
    "    matrix[1, 1] = a\n",
    "    matrix[2, 2] = 1\n",
    "    corners_3d = np.dot(matrix, np.vstack([x_corners, y_corners, z_corners]))\n",
    "    corners_3d[0, :] = corners_3d[0, :] + center[0].cpu().detach().numpy()\n",
    "    corners_3d[1, :] = corners_3d[1, :] + center[1].cpu().detach().numpy()\n",
    "    corners_3d[2, :] = corners_3d[2, :] + center[2].cpu().detach().numpy()\n",
    "    return np.transpose(corners_3d) # from 3,8 -> 8,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMS and mAP -> IoU threshold: 0.5 (for NMS), 0.5 for pedestrian and 0.7 for car (mAP)\n",
    "def voc_ap(rec, prec, use_07_metric=False):\n",
    "    \"\"\"Compute VOC AP given precision and recall. If use_07_metric is true, uses\n",
    "    the VOC 07 11-point method (default:False).\n",
    "    \"\"\"\n",
    "    if use_07_metric:\n",
    "        # 11 point metric\n",
    "        ap = 0.\n",
    "        for t in np.arange(0., 1.1, 0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0 \n",
    "            else:\n",
    "                p = np.max(prec[rec >= t])\n",
    "            ap = ap + p / 11.\n",
    "    else:\n",
    "        # correct AP calculation\n",
    "        # first append sentinel values at the end\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "        # compute the precision envelope\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "        \n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "        # and sum (\\Delta recall) * prec\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "def nms(y, proposals):\n",
    "    # y -> label -> 1class, 3center, 1r+1cangle, 1+3size, 6min maxcorners, 3mean_size, 1difficulty\n",
    "    # proposal -> 2objectness, 4class, 3center, 12*2angle, 3*4size (reg first for angle and cls first for size)\n",
    "    B, S = proposals.shape[0], proposals.shape[2] # batch size, num of proposals\n",
    "    objectness_scores = F.softmax(proposals[:,0:2,:], dim=1)\n",
    "    positive_objectness_scores = objectness_scores[:,1,:].view(B, -1)\n",
    "    class_scores = F.softmax(proposals[:,2:6,:], dim=1)\n",
    "    predict_classes = torch.max(class_scores, dim=1)[1]\n",
    "    car_ap = []\n",
    "    pe_ap = []\n",
    "    cy_ap = []\n",
    "    \n",
    "    for i in range(B): # every batch\n",
    "        object_sorted_scores, object_sorted_index = torch.sort(positive_objectness_scores[i])\n",
    "        original = object_sorted_index.cpu().detach().numpy().tolist()\n",
    "        keep = []\n",
    "        while len(original) > 0:\n",
    "            keep.append(original[0])\n",
    "            max_box = get_bb_box(proposals[i,6:9,original[0]], proposals[i,9:33,original[0]], proposals[i,33:45,original[0]])\n",
    "            del(original[0]) # remove the first item of original\n",
    "            for j in range(len(original)): # iterate the rest\n",
    "                if j < len(original): # avoid overflow\n",
    "                    box = get_bb_box(proposals[i,6:9,original[j]], proposals[i,9:33,original[j]], proposals[i,33:45,original[j]])\n",
    "                    if bboxIoU(max_box, box) > 0.5:\n",
    "                        del(original[j]) # delete the jth element\n",
    "                        j -= 1\n",
    "\n",
    "        keep = np.array(keep)\n",
    "        _, idx1, _, _ = nn_distance(proposals[i,6:9,keep].view(1,-1,3), y[i,:,1:4].view(1,-1,3))\n",
    "        \n",
    "        car_scores = []\n",
    "        car_index = []\n",
    "        car_keep_index = []\n",
    "        pe_scores = []\n",
    "        pe_index = []\n",
    "        pe_keep_index = []\n",
    "        cy_scores = []\n",
    "        cy_index = []\n",
    "        cy_keep_index = []\n",
    "        for j in range(1, 4): # iterate 3 classes\n",
    "            for k in range(keep.shape[0]):\n",
    "                if predict_classes[i,keep[k]] == j and j == 1:\n",
    "                    car_scores.append(class_scores[i,1,keep[k]])\n",
    "                    car_index.append(keep[k])\n",
    "                    car_keep_index.append(k)\n",
    "                elif predict_classes[i,keep[k]] == j and j == 2:\n",
    "                    pe_scores.append(class_scores[i,2,keep[k]])\n",
    "                    pe_index.append(keep[k])\n",
    "                    pe_keep_index.append(k)\n",
    "                elif predict_classes[i,keep[k]] == j and j == 3:\n",
    "                    cy_scores.append(class_scores[i,3,keep[k]])\n",
    "                    cy_index.append(keep[k])\n",
    "                    cy_keep_index.append(k)\n",
    "        \n",
    "        sorted_car, sorted_car_index = torch.sort(torch.FloatTensor(car_scores))\n",
    "        sorted_pe, sorted_pe_index = torch.sort(torch.FloatTensor(pe_scores))\n",
    "        sorted_cy, sorted_cy_index = torch.sort(torch.FloatTensor(cy_scores))\n",
    "\n",
    "        num_gt_car = 0\n",
    "        num_gt_pe = 0\n",
    "        num_gt_cy = 0\n",
    "        for j in range(y[i].shape[0]):\n",
    "            if y[i,j,0] == 1:\n",
    "                num_gt_car += 1\n",
    "            elif y[i,j,0] == 2:\n",
    "                num_gt_pe += 1\n",
    "            elif y[i,j,0] == 3:\n",
    "                num_gt_cy += 1\n",
    "        \n",
    "        # car p/r\n",
    "        car_precision = []\n",
    "        car_recall = []\n",
    "        tp = 1e-6\n",
    "        fp = 1e-6\n",
    "        fn = num_gt_car + 1e-6\n",
    "        detected_car_index = []\n",
    "        for j in range(sorted_car.shape[0]):\n",
    "            bbox1 = get_bb_box(proposals[i,6:9,car_index[sorted_car_index[j]]],proposals[i,9:33,car_index[sorted_car_index[j]]],proposals[i,33:45,car_index[sorted_car_index[j]]])\n",
    "            bbox2 = get_bb_box(y[i,idx1[0,car_keep_index[sorted_car_index[j]]].long(),1:4], y[i,idx1[0,car_keep_index[sorted_car_index[j]]].long(),4:6],y[i,idx1[0,car_keep_index[sorted_car_index[j]]].long(),6:10])\n",
    "            iou = bboxIoU(bbox1, bbox2)\n",
    "            if y[i,idx1[0,car_keep_index[sorted_car_index[j]]].long(),0] == 1 and iou >= 0.7:\n",
    "                is_detected_before = False\n",
    "                for k in range(len(detected_car_index)):\n",
    "                    if detected_car_index[k] == idx1[0,car_keep_index[sorted_car_index[j]]]:\n",
    "                        is_detected_before = True\n",
    "                        break\n",
    "                if is_detected_before == False:\n",
    "                    tp += 1\n",
    "                    fn -= 1\n",
    "                    detected_car_index.append(idx1[0,car_keep_index[sorted_car_index[j]]])\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            car_precision.append(tp / (tp + fp))\n",
    "            car_recall.append(tp / (tp + fn))\n",
    "        car_precision = np.array(car_precision)\n",
    "        car_recall = np.array(car_recall)\n",
    "        car_ap.append(voc_ap(car_recall, car_precision))\n",
    "\n",
    "        # pedestrian p/r\n",
    "        pe_precision = []\n",
    "        pe_recall = []\n",
    "        tp = 1e-6\n",
    "        fp = 1e-6\n",
    "        fn = num_gt_pe + 1e-6\n",
    "        detected_pe_index = []\n",
    "        for j in range(sorted_pe.shape[0]):\n",
    "            bbox1 = get_bb_box(proposals[i,6:9,pe_index[sorted_pe_index[j]]],proposals[i,9:33,pe_index[sorted_pe_index[j]]],proposals[i,33:45,pe_index[sorted_pe_index[j]]])\n",
    "            bbox2 = get_bb_box(y[i,idx1[0,pe_keep_index[sorted_pe_index[j]]].long(),1:4], y[i,idx1[0,pe_keep_index[sorted_pe_index[j]]].long(),4:6],y[i,idx1[0,pe_keep_index[sorted_pe_index[j]]].long(),6:10])\n",
    "            iou = bboxIoU(bbox1, bbox2)\n",
    "            if y[i,idx1[0,pe_keep_index[sorted_pe_index[j]]].long(),0] == 2 and iou >= 0.5:\n",
    "                is_detected_before = False\n",
    "                for k in range(len(detected_pe_index)):\n",
    "                    if detected_pe_index[k] == idx1[0,pe_keep_index[sorted_pe_index[j]]]:\n",
    "                        is_detected_before = True\n",
    "                        break\n",
    "                if is_detected_before == False:\n",
    "                    tp += 1\n",
    "                    fn -= 1\n",
    "                    detected_pe_index.append(idx1[0,pe_keep_index[sorted_pe_index[j]]])\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            pe_precision.append(tp / (tp + fp))\n",
    "            pe_recall.append(tp / (tp + fn))\n",
    "        pe_precision = np.array(pe_precision)\n",
    "        pe_recall = np.array(pe_recall)\n",
    "        pe_ap.append(voc_ap(pe_recall, pe_precision))\n",
    "\n",
    "        # cyclist p/r\n",
    "        cy_precision = []\n",
    "        cy_recall = []\n",
    "        tp = 1e-6\n",
    "        fp = 1e-6\n",
    "        fn = num_gt_cy + 1e-6\n",
    "        detected_cy_index = []\n",
    "        for j in range(sorted_cy.shape[0]):\n",
    "            bbox1 = get_bb_box(proposals[i,6:9,cy_index[sorted_cy_index[j]]],proposals[i,9:33,cy_index[sorted_cy_index[j]]],proposals[i,33:45,cy_index[sorted_cy_index[j]]])\n",
    "            bbox2 = get_bb_box(y[i,idx1[0,cy_keep_index[sorted_cy_index[j]]].long(),1:4], y[i,idx1[0,cy_keep_index[sorted_cy_index[j]]].long(),4:6],y[i,idx1[0,cy_keep_index[sorted_cy_index[j]]].long(),6:10])\n",
    "            iou = bboxIoU(bbox1, bbox2)\n",
    "            if y[i,idx1[0,cy_keep_index[sorted_cy_index[j]]].long(),0] == 3 and iou >= 0.5:\n",
    "                is_detected_before = False\n",
    "                for k in range(len(detected_cy_index)):\n",
    "                    if detected_cy_index[k] == idx1[0,cy_keep_index[sorted_cy_index[j]]]:\n",
    "                        is_detected_before = True\n",
    "                        break\n",
    "                if is_detected_before == False:\n",
    "                    tp += 1\n",
    "                    fn -= 1\n",
    "                    detected_cy_index.append(idx1[0,cy_keep_index[sorted_cy_index[j]]])\n",
    "                else:\n",
    "                    fp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            cy_precision.append(tp / (tp + fp))\n",
    "            cy_recall.append(tp / (tp + fn))\n",
    "        cy_precision = np.array(cy_precision)\n",
    "        cy_recall = np.array(cy_recall)\n",
    "        cy_ap.append(voc_ap(cy_recall, cy_precision))\n",
    "\n",
    "    return np.mean(np.array(car_ap)), np.mean(np.array(pe_ap)), np.mean(np.array(cy_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votenet = VoteNet(num_proposals=512, radius=1, num_neighbors=64, vote_dimension=259, mlp=[128,128,128], num_classes=4)\n",
    "votenet.to(device)\n",
    "votenet.load_state_dict(torch.load(\"votenet_1\"))\n",
    "votenet.eval()\n",
    "car_ap = []\n",
    "pe_ap = []\n",
    "cy_ap = []\n",
    "for i, data in enumerate(val_dataloader, 0):\n",
    "    X_val, y_val = data2\n",
    "    X_val = X_val.permute(0, 2, 1)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    _, _, _, proposals = votenet(X_val)\n",
    "    car_ap_temp, pe_ap_temp, cy_ap_temp = nms(y_val, proposals)\n",
    "    car_ap.append(car_ap_temp)\n",
    "    pe_ap.append(pe_ap_temp)\n",
    "    cy_ap.append(cy_ap_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.array(car_ap)))\n",
    "print(np.mean(np.array(pe_ap)))\n",
    "print(np.mean(np.array(cy_ap)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
